{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Aspect-Based Sentiment Analysis Pipeline \n",
    "\n",
    "This notebook combines preprocessing, aspect extraction, and sentiment analysis for Vietnamese customer feedback data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "import re\n",
    "import emoji\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import openai\n",
    "from collections import Counter\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Configure OpenAI settings for local LM Studio\n",
    "openai.api_base = 'http://localhost:1234/v1'\n",
    "openai.api_key = ''\n",
    "\n",
    "# Load PhoBERT model for sentiment analysis\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing URLs\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with URLs removed\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing HTML tags\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text without HTML tags\n",
    "    \"\"\"\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove special characters while preserving Vietnamese diacritics.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text with special characters\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with only alphanumeric and Vietnamese characters\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s\\u00C0-\\u1EF9.,!?]')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    \"\"\"Remove redundant whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text with extra spaces\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with normalized spacing\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Remove hashtags from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing hashtags\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with hashtags removed\n",
    "    \"\"\"\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_phone_numbers(text):\n",
    "    \"\"\"Replace phone numbers with [PHONE] token.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing phone numbers\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with phone numbers masked\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\b(?:\\+?84|0)(?:\\d{9,10})\\b', '[PHONE]', text)\n",
    "\n",
    "def is_meaningful(text, min_length=2, max_length=200, max_repetition_ratio=0.5, max_consonant_streak=5):\n",
    "    \"\"\"Check if text is meaningful based on various criteria.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to evaluate\n",
    "        min_length (int): Minimum character length\n",
    "        max_length (int): Maximum character length \n",
    "        max_repetition_ratio (float): Maximum ratio of most common character\n",
    "        max_consonant_streak (int): Maximum consecutive consonants allowed\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text is meaningful, False otherwise\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    if len(cleaned_text) < min_length or len(cleaned_text) > max_length:\n",
    "        return False\n",
    "    \n",
    "    if re.search(r'[bcdfghjklmnpqrstvwxyz]{' + str(max_consonant_streak) + ',}', cleaned_text):\n",
    "        return False\n",
    "    \n",
    "    char_counts = Counter(cleaned_text)\n",
    "    most_common_char_count = char_counts.most_common(1)[0][1]\n",
    "    repetition_ratio = most_common_char_count / len(cleaned_text)\n",
    "    \n",
    "    if repetition_ratio > max_repetition_ratio:\n",
    "        return False\n",
    "    \n",
    "    for pattern_length in range(2, 6):\n",
    "        for i in range(len(cleaned_text) - pattern_length * 2):\n",
    "            pattern = cleaned_text[i:i+pattern_length]\n",
    "            if pattern == cleaned_text[i+pattern_length:i+pattern_length*2]:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Apply all cleaning operations to text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Fully cleaned text\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_phone_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LM Studio API Integration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, max_retries=3):\n",
    "    \"\"\"Get completion from local LM Studio API.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Input prompt for the model\n",
    "        max_retries (int): Number of retries on failure\n",
    "        \n",
    "    Returns:\n",
    "        str: Model completion response or None on failure\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"local model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    return None\n",
    "\n",
    "def clean_and_validate_response(response):\n",
    "    \"\"\"Clean and validate JSON response from LM Studio.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Raw response from model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Parsed JSON response or None if invalid\n",
    "    \"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            json_start = response.find('{')\n",
    "            json_end = response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                return json.loads(response[json_start:json_end])\n",
    "        except:\n",
    "            pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment of text using PhoBERT model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text for sentiment analysis\n",
    "        \n",
    "    Returns:\n",
    "        dict: Sentiment probabilities for negative, positive, neutral\n",
    "    \"\"\"\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "        probs = out.logits.softmax(dim=-1).tolist()[0]\n",
    "    return {\n",
    "        \"Negative\": probs[0],\n",
    "        \"Positive\": probs[1],\n",
    "        \"Neutral\": probs[2]\n",
    "    }\n",
    "\n",
    "def get_dominant_sentiment(sentiment_dict):\n",
    "    \"\"\"Get the dominant sentiment from probability distribution.\n",
    "    \n",
    "    Args:\n",
    "        sentiment_dict (dict): Sentiment probabilities\n",
    "        \n",
    "    Returns:\n",
    "        str: Dominant sentiment category\n",
    "    \"\"\"\n",
    "    return max(sentiment_dict, key=sentiment_dict.get)\n",
    "\n",
    "def sentiment_to_polarity(sentiment):\n",
    "    \"\"\"Convert sentiment category to polarity label.\n",
    "    \n",
    "    Args:\n",
    "        sentiment (str): Sentiment category\n",
    "        \n",
    "    Returns:\n",
    "        str: Polarity label (NEG, POS, NEU)\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"negative\": \"NEG\",\n",
    "        \"positive\": \"POS\",\n",
    "        \"neutral\": \"NEU\"\n",
    "    }\n",
    "    return mapping.get(sentiment.lower(), \"NEU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feedback(feedback):\n",
    "    \"\"\"Process feedback by extracting and analyzing aspects.\n",
    "    \n",
    "    Args:\n",
    "        feedback (dict): Feedback entry with content and aspects\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed aspects with sentiment analysis\n",
    "    \"\"\"\n",
    "    content = feedback['Content']\n",
    "    results = []\n",
    "    \n",
    "    for aspect in feedback['Aspects']:\n",
    "        if len(aspect['AspectTerms']) == 0:\n",
    "            results.append({\n",
    "                'AspectCategory': aspect['AspectCategory'],\n",
    "                'AspectTerms': [],\n",
    "                'Polarity': None,\n",
    "                'DominantScore': None\n",
    "            })\n",
    "        else:\n",
    "            for term in aspect['AspectTerms']:\n",
    "                start = max(0, content.find(term) - 20)\n",
    "                end = min(len(content), content.find(term) + len(term) + 20)\n",
    "                context = content[start:end]\n",
    "                \n",
    "                sentiment = analyze_sentiment(context)\n",
    "                dominant_sentiment = get_dominant_sentiment(sentiment)\n",
    "                dominant_score = round(sentiment[dominant_sentiment], 3)\n",
    "                \n",
    "                results.append({\n",
    "                    'AspectCategory': aspect['AspectCategory'],\n",
    "                    'AspectTerms': term,\n",
    "                    'Polarity': sentiment_to_polarity(dominant_sentiment),\n",
    "                    'DominantScore': dominant_score\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_single_feedback(feedback):\n",
    "    \"\"\"Process single feedback entry through LM Studio.\n",
    "    \n",
    "    Args:\n",
    "        feedback (dict): Feedback entry\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed feedback with aspects or None\n",
    "    \"\"\"\n",
    "    prompt = f'''### Instruction:\n",
    "Analyze this feedback and extract aspects. Return ONLY a JSON object:\n",
    "{json.dumps(feedback, ensure_ascii=False)}\n",
    "\n",
    "Required format:\n",
    "{{\n",
    "    \"GeneralFeedbackID\": {feedback[\"GeneralFeedbackID\"]},\n",
    "    \"ID\": {feedback[\"ID\"]},\n",
    "    \"Content\": \"{feedback[\"Content\"]}\",\n",
    "    \"Aspects\": [\n",
    "        {{\n",
    "            \"AspectCategory\": \"Về Sản Phẩm\",\n",
    "            \"AspectTerms\": []\n",
    "        }},\n",
    "        {{\n",
    "            \"AspectCategory\": \"Về Dịch Vụ\", \n",
    "            \"AspectTerms\": []\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "### Response:'''\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    if response:\n",
    "        return clean_and_validate_response(response)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('warehouse/feedback/FeedbackDetail.csv', encoding='utf-8-sig')\n",
    "print('Initial records:', len(df))\n",
    "\n",
    "# Basic preprocessing\n",
    "df = df[['GeneralFeedbackID', 'Content']].drop_duplicates(subset='GeneralFeedbackID', keep='first')\n",
    "df['Content'] = df['Content'].astype(str)\n",
    "\n",
    "# Clean and tokenize\n",
    "df['Content'] = df['Content'].apply(clean_text)\n",
    "df['is_meaningful'] = df['Content'].apply(is_meaningful)\n",
    "df = df[df['is_meaningful']]\n",
    "df = df.drop(columns=['is_meaningful'])\n",
    "df = df[df['Content'].str.strip().astype(bool)]\n",
    "print('After cleaning:', len(df))\n",
    "\n",
    "# Process through LM Studio\n",
    "processed_results = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    feedback = {\n",
    "        'GeneralFeedbackID': row['GeneralFeedbackID'],\n",
    "        'ID': idx,  # Add ID here\n",
    "        'Content': row['Content']\n",
    "    }\n",
    "    processed = process_single_feedback(feedback)\n",
    "    if processed:\n",
    "        processed_results.append(processed)\n",
    "\n",
    "# Sentiment analysis\n",
    "final_results = []\n",
    "for feedback in processed_results:\n",
    "    processed_feedback = {\n",
    "        'GeneralFeedbackID': feedback['GeneralFeedbackID'],\n",
    "        'ID': feedback['ID'],\n",
    "        'Content': feedback['Content'],\n",
    "        'Aspects': process_feedback(feedback)\n",
    "    }\n",
    "    final_results.append(processed_feedback)\n",
    "\n",
    "# Save results\n",
    "with open('processed_feedback.json', 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "print('Processing complete. Results saved to processed_feedback.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
